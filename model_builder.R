# Build a N-gram language model based on N-grams generated by the MeTa Toolkit
# and Kneyser-Ney Smoothed probabilities (fixed discount)
# cf. accompanying README file for details

# Author: Renaud Dufour
# Date: 08/2015

library(dplyr)
library(tidyr)
library(ggplot2)
library(sqldf)
library(data.table)

source("helpers.R")

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# PART I. Read source files ####
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Source files consist of text data from news, blogs and twitter.
# About 1 Gb in total
# Tokenization into 1 to 4-grams is done externally with the MeTa toolkit
# (See README file for details regarding this process)
# note: I don't advice trying to do it with R ...

ngram_files <- "data/ngrams_from_meta/"
blog <- news <- tweet <- list()

# read blog data
blog$gram_1 <- readGramsFromMeta(paste0(path,"en_US.blogs.freq.1.txt"), order=1, min_count=2)
blog$gram_2 <- readGramsFromMeta(paste0(path,"en_US.blogs.freq.2.txt"), order=2, min_count=2)
blog$gram_3 <- readGramsFromMeta(paste0(path,"en_US.blogs.freq.3.txt"), order=3, min_count=2)
blog$gram_4 <- readGramsFromMeta(paste0(path,"en_US.blogs.freq.4.txt"), order=4, min_count=2)

# read news data
news$gram_1 <- readGramsFromMeta(paste0(path,"en_US.news.freq.1.txt"), order=1, min_count=2)
news$gram_2 <- readGramsFromMeta(paste0(path,"en_US.news.freq.2.txt"), order=2, min_count=2)
news$gram_3 <- readGramsFromMeta(paste0(path,"en_US.news.freq.3.txt"), order=3, min_count=2)
news$gram_4 <- readGramsFromMeta(paste0(path,"en_US.news.freq.4.txt"), order=4, min_count=2)

# read twitter data
tweet$gram_1 <- readGramsFromMeta(paste0(path,"en_US.twitter.freq.1.txt"), order=1, min_count=2)
tweet$gram_2 <- readGramsFromMeta(paste0(path,"en_US.twitter.freq.2.txt"), order=2, min_count=2)
tweet$gram_3 <- readGramsFromMeta(paste0(path,"en_US.twitter.freq.3.txt"), order=3, min_count=2)
tweet$gram_4 <- readGramsFromMeta(paste0(path,"en_US.twitter.freq.4.txt"), order=4, min_count=2)

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# PART II. Merge ngram tables & filter ####
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# We merge the above table by ngram order
# Then do some filtering keeping only alphabetic characters
# (still having difficulties handling properly unicodes with the combination windows/R/SQLite ...)
# Finally we keep only ngrams with count > 5 to limit memory footprint

gram_1 <- rbind(blog$gram_1, news$gram_1, tweet$gram_1) %>%
    group_by(word1) %>%
    summarise(count = sum(count)) %>%
    arrange(desc(count))

gram_2  <- rbind(blog$gram_2, news$gram_2, tweet$gram_2) %>%
    group_by_(.dots = lapply(c("word1","word2"), as.symbol)) %>%
    summarise(count = sum(count)) %>% ungroup %>%
    arrange(desc(count))

gram_3  <- rbind(blog$gram_3, news$gram_3, tweet$gram_3) %>%
    group_by_(.dots = lapply(c("word1","word2","word3"), as.symbol)) %>%
    summarise(count = sum(count)) %>% ungroup %>%
    arrange(desc(count))

gram_4  <- rbind(blog$gram_4, news$gram_4, tweet$gram_4) %>%
    group_by_(.dots = lapply(c("word1","word2","word3","word4"), as.symbol)) %>%
    summarise(count = sum(count)) %>% ungroup %>%
    arrange(desc(count))

# filter non alphabetic characters

pattern = "^[a-z]*$"
gram_1 <- gram_1 %>% filter(grepl(pattern = pattern, x = word1))
gram_2 <- gram_2 %>% filter(grepl(pattern = pattern, x = word1),
                            grepl(pattern = pattern, x = word2))
gram_3 <- gram_3 %>% filter(grepl(pattern = pattern, x = word1),
                            grepl(pattern = pattern, x = word2),
                            grepl(pattern = pattern, x = word3))
gram_4 <- gram_4 %>% filter(grepl(pattern = pattern, x = word1),
                            grepl(pattern = pattern, x = word2),
                            grepl(pattern = pattern, x = word3),
                            grepl(pattern = pattern, x = word4))

# Filter by count

threshold = 5

gram_1  <- gram_1  %>% filter(count > threshold)
gram_2  <- gram_2  %>% filter(count > threshold)
gram_3  <- gram_3  %>% filter(count > threshold)
gram_4  <- gram_4  %>% filter(count > threshold)

gram_1$count <- gram_1$count - threshold
gram_2$count <- gram_2$count - threshold
gram_3$count <- gram_3$count - threshold
gram_4$count <- gram_4$count - threshold

# Backup files & clean

save(gram_1, gram_2, gram_3, gram_4,  file = "data/ngrams.RData")
rm(list=setdiff(ls(), c("gram_1", "gram_2", "gram_3", "gram_4")))

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# PART III. Pre-compute probabilities ####
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# For each ngram table we compute the Maximul Likelihood estimates
# along with interpolated Kneyser-Ney propabilities (only the latter is used in final model)
# Computations are done using merging of data tables, which is the fatest wayI found so far (whole process should take just a few seconds)
# note: fixes related to start/end of sentence tags are only relevant if the latter are used, which is not the case here.

# set tables keys
setkey(gram_1, word1)
setkey(gram_2, word1, word2)
setkey(gram_3, word1, word2, word3)
setkey(gram_4, word1, word2, word3, word4)

# Compute some global variables
totalCounts <- sapply(list(gram_1,gram_2,gram_3,gram_4), function(x) sum(x$count))
gramCounts  <- sapply(list(gram_1,gram_2,gram_3,gram_4), nrow)

# Maximum Likelihood Estimate
gram_1[, qML := count/totalCounts[1]]
#gram_1["<s>",]$qML <- 0  # optional fix if start tags are used

gram_2[, qML := count/sum(count), by = word1]
gram_3[, qML := count/sum(count), by = .(word1,word2)]
gram_4[, qML := count/sum(count), by = .(word1,word2,word3)]

# Kneyser-Ney smoothed probabilities (max order is 4)

# a - continuation count N01 (up to max order - 1)

gram_1 <- merge(gram_1, gram_2[, .(N01 = .N), by=word2],
                by.x="word1", by.y="word2", all.x = TRUE)
gram_2 <- merge(gram_2, gram_3[, .(N01 = .N), by=.(word2,word3)],
                by.x=c("word1","word2"), by.y=c("word2","word3"), all.x = TRUE)
gram_3 <- merge(gram_3, gram_4[, .(N01 = .N), by=.(word2,word3,word4)],
                by.x=c("word1","word2","word3"), by.y=c("word2","word3","word4"), all.x = TRUE)

gram_1["<s>", N01:=count] # use real counts for sequences starting with <s>
gram_2["<s>", N01:=count]
gram_3["<s>", N01:=count]

gram_1[is.na(N01), N01:=1] # quick fix (NA appear because of filtering ngrams by count)
gram_2[is.na(N01), N01:=1]
gram_3[is.na(N01), N01:=1]

# b - continuation count N10 (up to max order - 1)

gram_1 <- merge(gram_1, gram_2[, .(N10 = .N), by=word1], all.x = TRUE)
gram_2 <- merge(gram_2, gram_3[, .(N10 = .N), by=.(word1,word2)], all.x = TRUE)
gram_3 <- merge(gram_3, gram_4[, .(N10 = .N), by=.(word1,word2,word3)], all.x = TRUE)

gram_1[word1 == "</s>", N10:=1]  # assume </s> can only be followed by a start tag
gram_2[word2 == "</s>", N10:=1]
gram_3[word3 == "</s>", N10:=1]

gram_1[is.na(N10), N10:=1] # quick fix (NA appear because of filtering ngrams by count)
gram_2[is.na(N10), N10:=1]
gram_3[is.na(N10), N10:=1]

# c - continuation count N010 (up to max order - 2)

gram_1 <- merge(gram_1, gram_2[, .(N010 = sum(N01)), by=word1], all.x = TRUE)
gram_2 <- merge(gram_2, gram_3[, .(N010 = sum(N01)), by=.(word1,word2)], all.x = TRUE)

gram_1[word1 == "</s>", N010:=N01]  # assume N010 = N01 since N10 is always 1 in this case
gram_2[word2 == "</s>", N010:=N01]

# d - discount values (fixed discount here, could also implement 'modified' kneyser-Ney by tweeking this part)

n11 <- gram_1[count=="1", .N]
n12 <- gram_2[count=="1", .N]
n13 <- gram_3[count=="1", .N]
n14 <- gram_4[count=="1", .N]
n21 <- gram_1[count=="2", .N]
n22 <- gram_2[count=="2", .N]
n23 <- gram_3[count=="2", .N]
n24 <- gram_4[count=="2", .N]

D <- unlist(c(n11/(n11+2*n21),
              n12/(n12+2*n22),
              n13/(n13+2*n23),
              n14/(n14+2*n24)))

# e - Compute the probabilities ( first compute lowest/higher/highest order results and then chain up)

# lowest order result
KN_low <- gram_1[, .(word1, qML, PKN=N01/sum(gram_1$N01, na.rm = TRUE))]

# higher order results
KN_higher2 <- merge(gram_2[, .(word1, word2, N01)], gram_1[, .(word1, N010, N10)])[
    , .(word1, word2, alpha = pmax(N01 - D[2], 0)/N010, gamma = D[2]*N10/N010)]

KN_higher3 <- merge(gram_3[, .(word1, word2, word3, N01)], gram_2[, .(word1, word2, N010, N10)])[
    , .(word1, word2, word3, alpha = pmax(N01 - D[3], 0)/N010, gamma = D[3]*N10/N010)]    

# highest order results
KN_highest2 <- merge(gram_2[, .(word1, word2, total_count = count)], gram_1[, .(word1, count, N10)])[
    , .(word1, word2, alpha = pmax(total_count - D[2], 0)/count, gamma = D[2]*N10/count)]

KN_highest3 <- merge(gram_3[, .(word1, word2, word3, total_count = count)], gram_2[, .(word1, word2, count, N10)])[
    , .(word1, word2, word3, alpha = pmax(total_count - D[3], 0)/count, gamma = D[3]*N10/count)]

KN_highest4 <- merge(gram_4[, .(word1, word2, word3, word4, total_count = count)], gram_3[, .(word1, word2, word3, count, N10)])[
    , .(word1, word2, word3, word4, alpha = pmax(total_count - D[4], 0)/count, gamma = D[4]*N10/count)]

# chain up from lowest to highest level
# (intermediate 'temp' calculations aim at avoiding recomputing the same results)

gram_1_final <- gram_1[, .(word1, qML, PKN = qML)]

gram_2_final <- merge(KN_highest2, KN_low[, .(word1, PKN)], by.x = "word2", by.y = "word1")[
    , .(word1, word2, PKN = alpha + gamma*PKN)]

temp2 <- merge(KN_higher2, KN_low[, .(word1, PKN)], by.x = "word2", by.y = "word1")[, .(word1, word2, PKN = alpha + gamma*PKN)]

gram_3_final <- merge(KN_highest3, temp2[, .(word1,word2, PKN)], by.x = c("word2","word3"), by.y = c("word1","word2"))[
    , .(word1, word2, word3, PKN = alpha + gamma*PKN)]

temp3 <- merge(KN_higher3, temp2[, .(word1,word2, PKN)], by.x = c("word2","word3"), by.y = c("word1","word2"))[
    , .(word1, word2, word3, PKN = alpha + gamma*PKN)]

gram_4_final <- merge(KN_highest4, temp3[, .(word1,word2,word3, PKN)], by.x = c("word2","word3","word4"), by.y = c("word1","word2","word3"))[
    , .(word1, word2, word3,word4, PKN = alpha + gamma*PKN)]

# and we end up with 4 interpolated KN models, one for each ngram order

# backup
save(gram_1_final, gram_2_final, gram_3_final, gram_4_final,  file = "data/ngrams_processed.RData")


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# PART III. Store in Database ####
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

database = "NWP.SQLite"

conn <- dbConnect(SQLite(), database)

dbWriteTable(conn, "gram_1", value = as.data.frame(gram_1_final), row.names = FALSE)
dbWriteTable(conn, "gram_2", value = as.data.frame(gram_2_final), row.names = FALSE)
dbWriteTable(conn, "gram_3", value = as.data.frame(gram_3_final), row.names = FALSE)
dbWriteTable(conn, "gram_4", value = as.data.frame(gram_4_final), row.names = FALSE)

cat("Database updated, now has the following tables: ",dbListTables(conn),"\n")
dbDisconnect(conn)

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#
# PART IV. Testing ####
#
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

source("predict.R")

input = "I would like a case of"
predict("E:/NWP/app v1/NWP.SQLite", input, method = 'PKN', npred = 1, max_order = 4)














